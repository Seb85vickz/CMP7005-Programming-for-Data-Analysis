{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Seb85vickz/CMP7005-Programming-for-Data-Analysis/blob/main/streamlit_app_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59866565",
      "metadata": {
        "id": "59866565"
      },
      "source": [
        "# Streamlit app: Advanced Multi-CSV Explorer\n",
        "\n",
        "This notebook provides the Streamlit app file and instructions to run it in a local environment or inside Colab (with limitations).\n",
        "\n",
        "Files created:\n",
        "\n",
        "- `/mnt/data/streamlit_app.py` : the Streamlit application\n",
        "\n",
        "You can download the app file directly from the links at the end of this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e08bb0a5"
      },
      "source": [
        "This cell will create the Streamlit application file at `/mnt/data/streamlit_app.py` with the provided Python code. This file contains the logic for the multi-CSV explorer."
      ],
      "id": "e08bb0a5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "new_cell_1",
        "outputId": "d9f3d2f2-b6fa-4db7-f574-0f97e6da4008"
      },
      "source": [
        "%%writefile /mnt/data/streamlit_app.py\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from io import StringIO\n",
        "import base64\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "st.set_page_config(layout='wide', page_title='Advanced Multi-CSV Explorer')\n",
        "\n",
        "def get_table_download_link(df, filename='data.csv', text='Download data as CSV'):\n",
        "    csv = df.to_csv(index=False)\n",
        "    b64 = base64.b64encode(csv.encode()).decode()  # some strings <-> bytes conversions necessary here\n",
        "    href = f'<a href=\"data:file/csv;base64,{b64}\" download=\"{filename}\">{text}</a>'\n",
        "    return href\n",
        "\n",
        "def df_info(df):\n",
        "    info = pd.DataFrame({\n",
        "        'Column': df.columns,\n",
        "        'Non-Null Count': df.count().values,\n",
        "        'Data Type': df.dtypes.values\n",
        "    })\n",
        "    return info\n",
        "\n",
        "def clean_column_name(col_name):\n",
        "    # Remove special characters and replace spaces with underscores\n",
        "    cleaned_name = re.sub(r'[^a-zA-Z0-9_]', '', col_name.replace(' ', '_'))\n",
        "    return cleaned_name\n",
        "\n",
        "@st.cache_data\n",
        "def load_data(uploaded_file):\n",
        "    # Try decoding with utf-8 first, then fall back to latin-1\n",
        "    try:\n",
        "        return pd.read_csv(uploaded_file)\n",
        "    except UnicodeDecodeError:\n",
        "        uploaded_file.seek(0)  # Reset file pointer\n",
        "        return pd.read_csv(uploaded_file, encoding='latin-1')\n",
        "\n",
        "# State management for file uploads\n",
        "if 'dfs' not in st.session_state:\n",
        "    st.session_state.dfs = {}\n",
        "if 'file_names' not in st.session_state:\n",
        "    st.session_state.file_names = {}\n",
        "\n",
        "# --- Sidebar for file uploads --- #\n",
        "st.sidebar.header('Upload CSV Files')\n",
        "\n",
        "uploaded_files = st.sidebar.file_uploader(\"Choose CSV files\", type=\"csv\", accept_multiple_files=True)\n",
        "\n",
        "if uploaded_files:\n",
        "    for uploaded_file in uploaded_files:\n",
        "        file_id = uploaded_file.name # Use filename as a unique ID\n",
        "        if file_id not in st.session_state.dfs:\n",
        "            df = load_data(uploaded_file)\n",
        "            if df is not None:\n",
        "                st.session_state.dfs[file_id] = df\n",
        "                st.session_state.file_names[file_id] = uploaded_file.name\n",
        "            else:\n",
        "                st.sidebar.error(f\"Could not load {uploaded_file.name}\")\n",
        "\n",
        "# Remove files button\n",
        "if st.session_state.dfs:\n",
        "    st.sidebar.subheader(\"Remove Loaded Files\")\n",
        "    files_to_remove = st.sidebar.multiselect(\"Select files to remove\", options=list(st.session_state.file_names.values()))\n",
        "    if st.sidebar.button(\"Remove Selected Files\"):\n",
        "        for fn in files_to_remove:\n",
        "            file_id_to_remove = next((id for id, name in st.session_state.file_names.items() if name == fn), None)\n",
        "            if file_id_to_remove and file_id_to_remove in st.session_state.dfs:\n",
        "                del st.session_state.dfs[file_id_to_remove]\n",
        "                del st.session_state.file_names[file_id_to_remove]\n",
        "        st.rerun()\n",
        "\n",
        "# --- Main content --- #\n",
        "st.title('Advanced Multi-CSV Explorer')\n",
        "\n",
        "if not st.session_state.dfs:\n",
        "    st.info(\"Please upload one or more CSV files from the sidebar to begin.\")\n",
        "else:\n",
        "    tab_names = list(st.session_state.file_names.values()) + ['Combined Analysis']\n",
        "    tabs = st.tabs(tab_names)\n",
        "\n",
        "    # Display individual dataframes and their summaries\n",
        "    for i, file_id in enumerate(st.session_state.dfs.keys()):\n",
        "        with tabs[i]:\n",
        "            st.header(f'File: {st.session_state.file_names[file_id]}')\n",
        "            df = st.session_state.dfs[file_id]\n",
        "\n",
        "            # Data Overview\n",
        "            st.subheader('Data Overview')\n",
        "            st.write(f'Shape: {df.shape[0]} rows, {df.shape[1]} columns')\n",
        "            st.write('First 5 Rows:')\n",
        "            st.dataframe(df.head())\n",
        "            st.write('Column Information:')\n",
        "            st.dataframe(df_info(df))\n",
        "\n",
        "            # Missing Values\n",
        "            st.subheader('Missing Values')\n",
        "            missing_values = df.isnull().sum()\n",
        "            missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
        "            missing_df = pd.DataFrame({\n",
        "                'Missing Count': missing_values,\n",
        "                'Missing Percentage': missing_percentage\n",
        "            }).sort_values(by='Missing Count', ascending=False)\n",
        "            st.dataframe(missing_df[missing_df['Missing Count'] > 0])\n",
        "\n",
        "            if not df.empty and df.select_dtypes(include=np.number).empty:\n",
        "                st.warning(\"This DataFrame contains no numeric columns for descriptive statistics.\")\n",
        "            elif not df.empty:\n",
        "                st.subheader('Descriptive Statistics (Numeric Columns)')\n",
        "                st.dataframe(df.describe())\n",
        "\n",
        "            st.markdown(get_table_download_link(df, filename=f'{st.session_state.file_names[file_id].replace(\".csv\", \"_processed.csv\")}', text='Download this DataFrame'), unsafe_allow_html=True)\n",
        "\n",
        "    # Combined Analysis Tab\n",
        "    with tabs[-1]:\n",
        "        st.header('Combined Data Analysis')\n",
        "\n",
        "        if len(st.session_state.dfs) < 2:\n",
        "            st.info(\"Upload at least two CSV files to perform combined analysis.\")\n",
        "        else:\n",
        "            # Dropdown to select columns for merging\n",
        "            st.subheader('Merge DataFrames')\n",
        "            selected_merge_cols = st.multiselect(\n",
        "                \"Select common columns to merge on (optional):\",\n",
        "                options=list(set.intersection(*[set(df.columns) for df in st.session_state.dfs.values()]))\n",
        "            )\n",
        "\n",
        "            combined_df = pd.DataFrame()\n",
        "            if not st.session_state.dfs:\n",
        "                st.error(\"No dataframes available for combination.\")\n",
        "            else:\n",
        "                dfs_to_combine = list(st.session_state.dfs.values())\n",
        "                file_names_list = list(st.session_state.file_names.values())\n",
        "\n",
        "                if selected_merge_cols:\n",
        "                    # Initial merge\n",
        "                    combined_df = dfs_to_combine[0]\n",
        "                    for k in range(1, len(dfs_to_combine)):\n",
        "                        combined_df = pd.merge(combined_df, dfs_to_combine[k], on=selected_merge_cols, how='outer', suffixes=(f'_{file_names_list[k-1].replace(\".csv\", \"\")}', f'_{file_names_list[k].replace(\".csv\", \"\")}{k}'))\n",
        "                else:\n",
        "                    # Concatenate if no merge columns selected or merge not possible\n",
        "                    try:\n",
        "                        combined_df = pd.concat(dfs_to_combine, ignore_index=True)\n",
        "                    except Exception as e:\n",
        "                        st.warning(f\"Could not concatenate DataFrames directly due to differing columns. Using first DataFrame for initial combined analysis. Error: {e}\")\n",
        "                        combined_df = dfs_to_combine[0] # Fallback to first DF if concat fails\n",
        "\n",
        "            if not combined_df.empty:\n",
        "                st.write(\"Combined DataFrame Head:\")\n",
        "                st.dataframe(combined_df.head())\n",
        "\n",
        "                st.subheader('Correlation Heatmap (Combined Numeric Data)')\n",
        "                numeric_cols_combined = combined_df.select_dtypes(include=np.number).columns\n",
        "                if len(numeric_cols_combined) > 1:\n",
        "                    corr_matrix = combined_df[numeric_cols_combined].corr()\n",
        "                    fig_corr = px.imshow(corr_matrix, text_auto=True, color_continuous_scale='Viridis')\n",
        "                    st.plotly_chart(fig_corr, use_container_width=True)\n",
        "                else:\n",
        "                    st.info(\"Not enough numeric columns in the combined data for a correlation heatmap.\")\n",
        "\n",
        "                st.subheader('Distribution of Numeric Columns (Combined Data)')\n",
        "                numeric_cols = combined_df.select_dtypes(include=np.number).columns.tolist()\n",
        "                if numeric_cols:\n",
        "                    selected_numeric_col_dist = st.selectbox('Select a numeric column for distribution:', numeric_cols, key='combined_dist_col')\n",
        "                    if selected_numeric_col_dist:\n",
        "                        fig_dist = px.histogram(combined_df, x=selected_numeric_col_dist, marginal=\"box\", nbins=30, title=f'Distribution of {selected_numeric_col_dist}')\n",
        "                        st.plotly_chart(fig_dist, use_container_width=True)\n",
        "                else:\n",
        "                    st.info(\"No numeric columns in the combined data to display distributions.\")\n",
        "\n",
        "                st.subheader('Categorical Column Value Counts (Combined Data)')\n",
        "                categorical_cols = combined_df.select_dtypes(include='object').columns.tolist()\n",
        "                if categorical_cols:\n",
        "                    selected_cat_col_vc = st.selectbox('Select a categorical column for value counts:', categorical_cols, key='combined_cat_vc')\n",
        "                    if selected_cat_col_vc:\n",
        "                        value_counts = combined_df[selected_cat_col_vc].value_counts().reset_index()\n",
        "                        value_counts.columns = [selected_cat_col_vc, 'Count']\n",
        "                        fig_vc = px.bar(value_counts.head(20), x=selected_cat_col_vc, y='Count', title=f'Value Counts for {selected_cat_col_vc}')\n",
        "                        st.plotly_chart(fig_vc, use_container_width=True)\n",
        "                else:\n",
        "                    st.info(\"No categorical columns in the combined data to display value counts.\")\n",
        "\n",
        "                # Pairplot for numerical columns\n",
        "                if len(numeric_cols) > 1:\n",
        "                    st.subheader('Scatter Matrix (Pairplot) of Numeric Columns')\n",
        "                    pairplot_cols = st.multiselect('Select up to 5 numeric columns for pairplot:', numeric_cols, default=numeric_cols[:min(5, len(numeric_cols))], key='pairplot_cols')\n",
        "                    if pairplot_cols:\n",
        "                        try:\n",
        "                            fig_pair = px.scatter_matrix(combined_df, dimensions=pairplot_cols)\n",
        "                            st.plotly_chart(fig_pair, use_container_width=True)\n",
        "                        except Exception as e:\n",
        "                            st.error(f\"Error generating pairplot: {e}\")\n",
        "\n",
        "                st.markdown(get_table_download_link(combined_df, filename='combined_data_processed.csv', text='Download Combined DataFrame'), unsafe_allow_html=True)"
      ],
      "id": "new_cell_1",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /mnt/data/streamlit_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "1986e62e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1986e62e",
        "outputId": "89e45085-01b8-45eb-d8c9-aa82d64d9f66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "App exists: True\n",
            "----------------------------------------\n",
            "\n",
            "import streamlit as st\n",
            "import pandas as pd\n",
            "import plotly.express as px\n",
            "import plotly.graph_objects as go\n",
            "from io import StringIO\n",
            "import base64\n",
            "from collections import defaultdict\n",
            "import re\n",
            "import numpy as np\n",
            "from sklearn.preprocessing import MinMaxScaler\n",
            "\n",
            "st.set_page_config(layout='wide', page_title='Advanced Multi-CSV Explorer')\n",
            "\n",
            "def get_table_download_link(df, filename='data.csv', text='Download data as CSV'):\n",
            "    csv = df.to_csv(index=False)\n",
            "    b64 = base64.b64encode(csv.encode()).decode()  # some strings <-> bytes conversions necessary here\n",
            "    href = f'<a href=\"data:file/csv;base64,{b64}\" download=\"{filename}\">{text}</a>'\n",
            "    return href\n",
            "\n",
            "def df_info(df):\n",
            "    info = pd.DataFrame({\n",
            "        'Column': df.columns,\n",
            "        'Non-Null Count': df.count().values,\n",
            "        'Data Type': df.dtypes.values\n",
            "    })\n",
            "    return info\n",
            "\n",
            "def clean_column_name(col_name):\n",
            "    # Remove special characters and replace spaces with underscores\n",
            "    cleaned_name = re.sub(r'[^a-zA-Z0-9_]', '', col_name.replace(' ', '_'))\n",
            "    return cleaned_name\n",
            "\n",
            "@st.cache_data\n",
            "def load_data(uploaded_file):\n",
            "    # Try decoding with utf-8 first, then fall back to latin-1\n",
            "    try:\n",
            "        return pd.read_csv(uploaded_file)\n",
            "    except UnicodeDecodeError:\n",
            "        uploaded_file.seek(0)  # Reset file pointer\n",
            "        return pd.read_csv(uploaded_file, encoding='latin-1')\n",
            "\n",
            "# State management for file uploads\n",
            "if 'dfs' not in st.session_state:\n",
            "    st.session_state.dfs = {}\n",
            "if 'file_names' not in st.session_state:\n",
            "    st.session_state.file_names = {}\n",
            "\n",
            "# --- Sidebar for file uploads --- #\n",
            "st.sidebar.header('Upload CSV Files')\n",
            "\n",
            "uploaded_files = st.sidebar.file_uploader(\"Choose CSV files\", type=\"csv\", accept_multiple_files=True)\n",
            "\n",
            "if uploaded_files:\n",
            "    for uploaded_file in uploaded_files:\n",
            "        file_id = uploaded_file.name # Use filename as a unique ID\n",
            "        if file_id not in st.session_state.dfs:\n",
            "            df = load_data(uploaded_file)\n",
            "            if df is not None:\n",
            "                st.session_state.dfs[file_id] = df\n",
            "                st.session_state.file_names[file_id] = uploaded_file.name\n",
            "            else:\n",
            "                st.sidebar.error(f\"Could not load {uploaded_file.name}\")\n",
            "\n",
            "# Remove files button\n",
            "if st.session_state.dfs:\n",
            "    st.sidebar.subheader(\"Remove Loaded Files\")\n",
            "    files_to_remove = st.sidebar.multiselect(\"Select files to remove\", options=list(st.session_state.file_names.values()))\n",
            "    if st.sidebar.button(\"Remove Selected Files\"):\n",
            "        for fn in files_to_remove:\n",
            "            file_id_to_remove = next((id for id, name in st.session_state.file_names.items() if name == fn), None)\n",
            "            if file_id_to_remove and file_id_to_remove in st.session_state.dfs:\n",
            "                del st.session_state.dfs[file_id_to_remove]\n",
            "                del st.session_state.file_names[file_id_to_remove]\n",
            "        st.rerun()\n",
            "\n",
            "# --- Main content --- #\n",
            "st.title('Advanced Multi-CSV Explorer')\n",
            "\n",
            "if not st.session_state.dfs:\n",
            "    st.info(\"Please upload one or more CSV files from the sidebar to begin.\")\n",
            "else:\n",
            "    tab_names = list(st.session_state.file_names.values()) + ['Combined Analysis']\n",
            "    tabs = st.tabs(tab_names)\n",
            "\n",
            "    # Display individual dataframes and their summaries\n",
            "    for i, file_id in enumerate(st.session_state.dfs.keys()):\n",
            "        with tabs[i]:\n",
            "            st.header(f'File: {st.session_state.file_names[file_id]}')\n",
            "            df = st.session_state.dfs[file_id]\n",
            "\n",
            "            # Data Overview\n",
            "            st.subheader('Data Overview')\n",
            "            st.write(f'Shape: {df.shape[0]} rows, {df.shape[1]} columns')\n",
            "            st.write('First 5 Rows:')\n",
            "            st.dataframe(df.head())\n",
            "            st.write('Column Information:')\n",
            "            st.dataframe(df_info(df))\n",
            "\n",
            "            # Missing Values\n",
            "            st.subheader('Missing Values')\n",
            "            missing_values = df.isnull().sum()\n",
            "            missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
            "            missing_df = pd.DataFrame({\n",
            "                'Missing Count': missing_values,\n",
            "                'Missing Percentage': missing_percentage\n",
            "            }).sort_values(by='Missing Count', ascending=False)\n",
            "            st.dataframe(missing_df[missing_df['Missing Count'] > 0])\n",
            "\n",
            "            if not df.empty and df.select_dtypes(include=np.number).empty:\n",
            "                st.warning(\"This DataFrame contains no numeric columns for descriptive statistics.\")\n",
            "            elif not df.empty:\n",
            "                st.subheader('Descriptive Statistics (Numeric Columns)')\n",
            "                st.dataframe(df.describe())\n",
            "            \n",
            "            st.markdown(get_table_download_link(df, filename=f'{st.session_state.file_names[file_id].replace(\".csv\", \"_processed.csv\")}', text='Download this DataFrame'), unsafe_allow_html=True)\n",
            "\n",
            "    # Combined Analysis Tab\n",
            "    with tabs[-1]:\n",
            "        st.header('Combined Data Analysis')\n",
            "\n",
            "        if len(st.session_state.dfs) < 2:\n",
            "            st.info(\"Upload at least two CSV files to perform combined analysis.\")\n",
            "        else:\n",
            "            # Dropdown to select columns for merging\n",
            "            st.subheader('Merge DataFrames')\n",
            "            selected_merge_cols = st.multiselect(\n",
            "                \"Select common columns to merge on (optional):\",\n",
            "                options=list(set.intersection(*[set(df.columns) for df in st.session_state.dfs.values()]))\n",
            "            )\n",
            "\n",
            "            combined_df = pd.DataFrame()\n",
            "            if not st.session_state.dfs:\n",
            "                st.error(\"No dataframes available for combination.\")\n",
            "            else:\n",
            "                dfs_to_combine = list(st.session_state.dfs.values())\n",
            "                file_names_list = list(st.session_state.file_names.values())\n",
            "\n",
            "                if selected_merge_cols:\n",
            "                    # Initial merge\n",
            "                    combined_df = dfs_to_combine[0]\n",
            "                    for k in range(1, len(dfs_to_combine)):\n",
            "                        combined_df = pd.merge(combined_df, dfs_to_combine[k], on=selected_merge_cols, how='outer', suffixes=(f'_{file_names_list[k-1].replace(\".csv\", \"\")}', f'_{file_names_list[k].replace(\".csv\", \"\")}{k}'))\n",
            "                else:\n",
            "                    # Concatenate if no merge columns selected or merge not possible\n",
            "                    try:\n",
            "                        combined_df = pd.concat(dfs_to_combine, ignore_index=True)\n",
            "                    except Exception as e:\n",
            "                        st.warning(f\"Could not concatenate DataFrames directly due to differing columns. Using first DataFrame for initial combined analysis. Error: {e}\")\n",
            "                        combined_df = dfs_to_combine[0] # Fallback to first DF if concat fails\n",
            "\n",
            "            if not combined_df.empty:\n",
            "                st.write(\"Combined DataFrame Head:\")\n",
            "                st.dataframe(combined_df.head())\n",
            "\n",
            "                st.subheader('Correlation Heatmap (Combined Numeric Data)')\n",
            "                numeric_cols_combined = combined_df.select_dtypes(include=np.number).columns\n",
            "                if len(numeric_cols_combined) > 1:\n",
            "                    corr_matrix = combined_df[numeric_cols_combined].corr()\n",
            "                    fig_corr = px.imshow(corr_matrix, text_auto=True, color_continuous_scale='Viridis')\n",
            "                    st.plotly_chart(fig_corr, use_container_width=True)\n",
            "                else:\n",
            "                    st.info(\"Not enough numeric columns in the combined data for a correlation heatmap.\")\n",
            "\n",
            "                st.subheader('Distribution of Numeric Columns (Combined Data)')\n",
            "                numeric_cols = combined_df.select_dtypes(include=np.number).columns.tolist()\n",
            "                if numeric_cols:\n",
            "                    selected_numeric_col_dist = st.selectbox('Select a numeric column for distribution:', numeric_cols, key='combined_dist_col')\n",
            "                    if selected_numeric_col_dist:\n",
            "                        fig_dist = px.histogram(combined_df, x=selected_numeric_col_dist, marginal=\"box\", nbins=30, title=f'Distribution of {selected_numeric_col_dist}')\n",
            "                        st.plotly_chart(fig_dist, use_container_width=True)\n",
            "                else:\n",
            "                    st.info(\"No numeric columns in the combined data to display distributions.\")\n",
            "\n",
            "                st.subheader('Categorical Column Value Counts (Combined Data)')\n",
            "                categorical_cols = combined_df.select_dtypes(include='object').columns.tolist()\n",
            "                if categorical_cols:\n",
            "                    selected_cat_col_vc = st.selectbox('Select a categorical column for value counts:', categorical_cols, key='combined_cat_vc')\n",
            "                    if selected_cat_col_vc:\n",
            "                        value_counts = combined_df[selected_cat_col_vc].value_counts().reset_index()\n",
            "                        value_counts.columns = [selected_cat_col_vc, 'Count']\n",
            "                        fig_vc = px.bar(value_counts.head(20), x=selected_cat_col_vc, y='Count', title=f'Value Counts for {selected_cat_col_vc}')\n",
            "                        st.plotly_chart(fig_vc, use_container_width=True)\n",
            "                else:\n",
            "                    st.info(\"No categorical columns in the combined data to display value counts.\")\n",
            "\n",
            "                # Pairplot for numerical columns\n",
            "                if len(numeric_cols) > 1:\n",
            "                    st.subheader('Scatter Matrix (Pairplot) of Numeric Columns')\n",
            "                    pairplot_cols = st.multiselect('Select up to 5 numeric columns for pairplot:', numeric_cols, default=numeric_cols[:min(5, len(numeric_cols))], key='pairplot_cols')\n",
            "                    if pairplot_cols:\n",
            "                        try:\n",
            "                            fig_pair = px.scatter_matrix(combined_df, dimensions=pairplot_cols)\n",
            "                            st.plotly_chart(fig_pair, use_container_width=True)\n",
            "                        except Exception as e:\n",
            "                            st.error(f\"Error generating pairplot: {e}\")\n",
            "                \n",
            "                st.markdown(get_table_download_link(combined_df, filename='combined_data_processed.csv', text='Download Combined DataFrame'), unsafe_allow_html=True)\n"
          ]
        }
      ],
      "source": [
        "# Show the created streamlit app path and preview the first 200 lines\n",
        "from pathlib import Path\n",
        "p = Path('/mnt/data/streamlit_app.py')\n",
        "print('App exists:', p.exists())\n",
        "print('-'*40)\n",
        "print('\\n'.join(open(p).read().splitlines()[:200]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5baa6629",
      "metadata": {
        "id": "5baa6629"
      },
      "source": [
        "## How to run the Streamlit app locally\n",
        "\n",
        "1. Ensure you have Python 3.8+ and pip installed.\n",
        "2. Install required packages:\n",
        "\n",
        "```\n",
        "pip install streamlit pandas plotly scikit-learn\n",
        "```\n",
        "\n",
        "3. Run the app from the folder that contains your CSV files (or set the Data directory in the sidebar):\n",
        "\n",
        "```\n",
        "streamlit run /mnt/data/streamlit_app.py\n",
        "```\n",
        "\n",
        "If you're using Google Colab, running Streamlit requires extra steps (using `ngrok` or `localtunnel`) and is not covered here. It's recommended to run locally for full interactivity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "493b88cf",
      "metadata": {
        "id": "493b88cf"
      },
      "source": [
        "## Files created\n",
        "Use the links below to download the app file and the notebook itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f02cd5bf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f02cd5bf",
        "outputId": "61a596b8-f878-442a-fc40-00d1ec780a15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit app file:\n",
            "/mnt/data/streamlit_app.py\n",
            "\n",
            "Notebook file:\n",
            "/mnt/data/streamlit_app_notebook.ipynb\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import FileLink, FileLinks\n",
        "print('Streamlit app file:')\n",
        "print(FileLink('/mnt/data/streamlit_app.py'))\n",
        "print('\\nNotebook file:')\n",
        "print(FileLink('/mnt/data/streamlit_app_notebook.ipynb'))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}